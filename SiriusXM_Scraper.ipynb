{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/froggeric/Scrap-SiriusXM/blob/main/SiriusXM_Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jv0EAcVinQ-0"
      },
      "outputs": [],
      "source": [
        "# Step 2: Install Necessary Libraries (Run this cell first in Colab)\n",
        "!pip install requests beautifulsoup4 pandas lxml --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "yEOo-yPGnQ-1",
        "outputId": "c8da1e0e-fbc4-4d74-b5a2-8cc535da58bd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-3-ae1a3033a841>, line 288)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-ae1a3033a841>\"\u001b[0;36m, line \u001b[0;32m288\u001b[0m\n\u001b[0;31m    url = result_item.get('link'); if not url: continue\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# --- Imports ---\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import urllib.parse\n",
        "import json\n",
        "import os\n",
        "try:\n",
        "    # To use Colab Secrets for API Keys\n",
        "    from google.colab import userdata\n",
        "    SECRETS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SECRETS_AVAILABLE = False\n",
        "    # print(\"Warning: 'google.colab.userdata' not available...\") # Keep silent unless debugging\n",
        "\n",
        "# ==============================================================================\n",
        "# === USER CONFIGURATION AREA ==================================================\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Input / Output Files ---\n",
        "# USER ACTION: Create this CSV file and upload to Colab or specify path.\n",
        "# Expected columns: 'Name' (optional), 'Role_Type' (e.g., \"DJ/Host\", \"Role\"), 'Associated_Channel' (optional)\n",
        "INPUT_CSV_FILE = \"targets.csv\"\n",
        "OUTPUT_CSV_FILE = \"siriusxm_contacts_output.csv\" # Output filename\n",
        "\n",
        "# --- Scraping Behavior ---\n",
        "REQUEST_DELAY = 2       # Seconds delay between direct web requests (be polite!)\n",
        "REQUEST_TIMEOUT = 15    # Seconds before timeout\n",
        "MAX_RETRIES = 2         # Number of retries after the initial attempt for network errors\n",
        "RETRY_DELAY = 5         # Seconds to wait before retrying a failed request\n",
        "SAVE_INTERVAL = 25      # Save progress to CSV every N records processed\n",
        "\n",
        "# --- API Keys / CSE ID ---\n",
        "# USER ACTION: Store these in Colab Secrets (Key icon in left sidebar)\n",
        "# Recommended Secret Names: GOOGLE_API_KEY, GOOGLE_CSE_ID\n",
        "\n",
        "# --- Web Request Headers ---\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
        "    'Accept-Language': 'en-US,en;q=0.9',\n",
        "}\n",
        "\n",
        "# --- Google API ---\n",
        "GOOGLE_SEARCH_API_URL = \"https://www.googleapis.com/customsearch/v1\"\n",
        "\n",
        "# --- CSS Selectors (USER MUST VERIFY/UPDATE THESE by inspecting live websites!) ---\n",
        "# These are plausible examples based on common structures but WILL likely need changes.\n",
        "\n",
        "# SiriusXM Channel Page Selectors (Example - Verify!)\n",
        "SIRIUSXM_CHANNELS_URL = \"https://www.siriusxm.com/channels\"\n",
        "SIRIUSXM_CHANNEL_LIST_SELECTOR = \"a[href*='/channels/']\" # Try finding links pointing to specific channels\n",
        "SIRIUSXM_CHANNEL_NAME_SELECTOR = \"h4, div[role='heading']\" # Try common heading tags or divs marked as headings\n",
        "SIRIUSXM_CHANNEL_URL_SELECTOR = None # Set to None if the LIST_SELECTOR is the 'a' tag itself.\n",
        "\n",
        "# LinkedIn Public Profile Selectors (Example - Verify!)\n",
        "LINKEDIN_TITLE_SELECTOR = \"title\" # Usually reliable\n",
        "\n",
        "# Social Media Profile Selectors (Example - Verify!)\n",
        "SOCIAL_BIO_SELECTOR_OG = \"meta[property='og:description']\" # Common, try first\n",
        "SOCIAL_BIO_SELECTOR_META = \"meta[name='description']\"      # Fallback\n",
        "\n",
        "# Follower counts - Static selectors usually FAIL. Requires JS/Selenium/API.\n",
        "SOCIAL_FOLLOWER_SELECTOR = None # Explicitly None as requests/BS4 cannot reliably get this\n",
        "\n",
        "# ==============================================================================\n",
        "# === END USER CONFIGURATION AREA ==============================================\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def get_soup(url):\n",
        "    \"\"\"\n",
        "    Fetches a URL and returns a BeautifulSoup object, with error handling and retries.\n",
        "    Retries on Timeout, ConnectionError, and general RequestException.\n",
        "    \"\"\"\n",
        "    soup = None\n",
        "    for attempt in range(MAX_RETRIES + 1):\n",
        "        try:\n",
        "            print(f\"Fetching: {url} (Attempt {attempt + 1}/{MAX_RETRIES + 1})\")\n",
        "            response = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)\n",
        "            response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "            soup = BeautifulSoup(response.content, 'lxml')\n",
        "            # print(f\"Successfully fetched: {url}\") # Reduce noise\n",
        "            break # Exit loop on success\n",
        "\n",
        "        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError, requests.exceptions.RequestException) as e:\n",
        "            print(f\"Warning: Network error fetching {url}: {e}\")\n",
        "            if attempt < MAX_RETRIES:\n",
        "                print(f\"Retrying in {RETRY_DELAY} seconds...\")\n",
        "                time.sleep(RETRY_DELAY)\n",
        "                continue # <<<--- CORRECTED: Added continue to proceed to next attempt\n",
        "            else:\n",
        "                print(f\"Error: Max retries reached for {url}. Giving up.\")\n",
        "                soup = None # Ensure None on final failure\n",
        "\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "             print(f\"Error: HTTP Error {e.response.status_code} fetching {url} (Not retrying).\")\n",
        "             soup = None # Ensure None on non-retryable HTTP error\n",
        "             break # Exit loop\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: Could not process {url}: {e} (Not retrying).\")\n",
        "            soup = None # Ensure None on other errors\n",
        "            break # Exit loop\n",
        "\n",
        "    # Delay slightly even if successful, more if failed? No, keep consistent delay after attempts.\n",
        "    time.sleep(REQUEST_DELAY / 2) # Slightly shorter delay after direct fetches ok\n",
        "    return soup\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Helper to strip whitespace and potentially clean up text.\"\"\"\n",
        "    return text.strip() if text else None\n",
        "\n",
        "\n",
        "# --- Search Functions ---\n",
        "def search_engine_lookup(query, api_key=None, cse_id=None, num_results=5):\n",
        "    \"\"\"\n",
        "    Executes a web search using the Google Custom Search JSON API with retries.\n",
        "    \"\"\"\n",
        "    # --- Get API Key and CSE ID securely ---\n",
        "    if not api_key:\n",
        "        if SECRETS_AVAILABLE:\n",
        "            try: api_key = userdata.get('GOOGLE_API_KEY')\n",
        "            except userdata.SecretNotFoundError: print(\"Error: Google API Key not found in Colab Secrets ('GOOGLE_API_KEY').\"); return []\n",
        "        else:\n",
        "            api_key = os.environ.get('GOOGLE_API_KEY');\n",
        "            if not api_key: print(\"Error: GOOGLE_API_KEY not found in environment variables.\"); return []\n",
        "    if not cse_id:\n",
        "        if SECRETS_AVAILABLE:\n",
        "            try: cse_id = userdata.get('GOOGLE_CSE_ID')\n",
        "            except userdata.SecretNotFoundError: print(\"Error: Google CSE ID not found in Colab Secrets ('GOOGLE_CSE_ID').\"); return []\n",
        "        else:\n",
        "            cse_id = os.environ.get('GOOGLE_CSE_ID');\n",
        "            if not cse_id: print(\"Error: GOOGLE_CSE_ID not found in environment variables.\"); return []\n",
        "\n",
        "    params = {'key': api_key, 'cx': cse_id, 'q': query, 'num': num_results}\n",
        "    results_found = []\n",
        "    response_json = None\n",
        "\n",
        "    for attempt in range(MAX_RETRIES + 1):\n",
        "        try:\n",
        "            print(f\"Executing Google API search for: '{query}' (Attempt {attempt + 1}/{MAX_RETRIES + 1})\")\n",
        "            response = requests.get(GOOGLE_SEARCH_API_URL, params=params, headers=HEADERS, timeout=REQUEST_TIMEOUT)\n",
        "            response.raise_for_status()\n",
        "            response_json = response.json() # Try decoding JSON here\n",
        "            # print(f\"Successfully got API response for: '{query}'\") # Reduce noise\n",
        "            break # Exit loop on success\n",
        "\n",
        "        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError, requests.exceptions.RequestException) as e:\n",
        "            print(f\"Warning: Network error during Google API search for '{query}': {e}\")\n",
        "            if attempt < MAX_RETRIES:\n",
        "                print(f\"Retrying in {RETRY_DELAY} seconds...\")\n",
        "                time.sleep(RETRY_DELAY)\n",
        "                continue # <<<--- CORRECTED: Added continue to proceed to next attempt\n",
        "            else:\n",
        "                print(f\"Error: Max retries reached for Google API search '{query}'. Giving up.\")\n",
        "                return [] # Return empty on final failure\n",
        "\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "             print(f\"Error: HTTP Error {e.response.status_code} during Google API search for '{query}' (Not retrying). Status: {e.response.text}\")\n",
        "             return []\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "             print(f\"Error: Could not decode JSON response from Google API for '{query}' (Not retrying). Response text: {response.text[:200]}...\")\n",
        "             return []\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: An unexpected error occurred during Google API search for '{query}': {e} (Not retrying).\")\n",
        "            return []\n",
        "\n",
        "    # --- Parse Response ---\n",
        "    if response_json:\n",
        "        if 'error' in response_json:\n",
        "            error_details = response_json['error'].get('message', 'Unknown API error')\n",
        "            print(f\"Error: Google API returned an application-level error: {error_details}\")\n",
        "            return []\n",
        "        items = response_json.get('items', [])\n",
        "        # print(f\"No search results found via API for '{query}'.\") # Only print if needed\n",
        "        for item in items:\n",
        "            link = item.get('link'); title = item.get('title'); snippet = item.get('snippet')\n",
        "            if link: results_found.append({'link': link, 'title': title, 'snippet': snippet})\n",
        "        print(f\"Found {len(results_found)} results via Google API for '{query}'.\")\n",
        "    else:\n",
        "        print(f\"No valid response received after retries for Google API search '{query}'.\")\n",
        "        return []\n",
        "\n",
        "    time.sleep(max(REQUEST_DELAY / 4, 0.5)) # Short delay after successful API call\n",
        "    return results_found\n",
        "\n",
        "def search_reddit(query, api_key=None, cse_id=None, num_results=3):\n",
        "    \"\"\"Searches Reddit using the Google Custom Search API by adding 'site:reddit.com'.\"\"\"\n",
        "    reddit_query = f\"site:reddit.com {query}\"\n",
        "    reddit_results = search_engine_lookup(query=reddit_query, api_key=api_key, cse_id=cse_id, num_results=num_results)\n",
        "    return reddit_results\n",
        "\n",
        "\n",
        "# --- Scraping Functions ---\n",
        "def scrape_siriusxm_channels(url):\n",
        "    \"\"\"Scrapes the main SiriusXM channels page for channel names and URLs.\"\"\"\n",
        "    print(\"Scraping SiriusXM Channel List...\")\n",
        "    soup = get_soup(url) # Uses retry logic\n",
        "    channels_data = []\n",
        "    if not soup: print(\"Failed to get SiriusXM channel page soup after retries.\"); return channels_data\n",
        "\n",
        "    # --- Using EXAMPLE selector - USER MUST VERIFY/UPDATE ---\n",
        "    channel_link_elements = soup.select(SIRIUSXM_CHANNEL_LIST_SELECTOR)\n",
        "    if not channel_link_elements: print(f\"Warning: SiriusXM channel selector '{SIRIUSXM_CHANNEL_LIST_SELECTOR}' not found.\"); return channels_data\n",
        "\n",
        "    processed_urls = set()\n",
        "    for element in channel_link_elements:\n",
        "        try:\n",
        "            channel_url_relative = element.get('href')\n",
        "            if not channel_url_relative or not channel_url_relative.startswith('/channels/'): continue\n",
        "            channel_url_absolute = urllib.parse.urljoin(url, channel_url_relative)\n",
        "            if channel_url_absolute in processed_urls: continue\n",
        "            processed_urls.add(channel_url_absolute)\n",
        "\n",
        "            # --- Using EXAMPLE selectors - USER MUST VERIFY/UPDATE ---\n",
        "            name_tag = element.select_one(SIRIUSXM_CHANNEL_NAME_SELECTOR)\n",
        "            name = clean_text(name_tag.get_text()) if name_tag else None\n",
        "            if not name: name = clean_text(element.get('aria-label'))\n",
        "            if not name:\n",
        "                img_tag = element.find('img');\n",
        "                if img_tag: name = clean_text(img_tag.get('alt'))\n",
        "            if not name and channel_url_relative:\n",
        "                 name_guess = channel_url_relative.split('/')[-1]\n",
        "                 if name_guess: name = name_guess.replace('-', ' ').title()\n",
        "\n",
        "            if name: channels_data.append({\"channel_name\": name, \"channel_url\": channel_url_absolute})\n",
        "            else: print(f\"Warning: Could not determine name for channel URL: {channel_url_absolute}\")\n",
        "        except Exception as e: print(f\"Error parsing a channel element: {e}\")\n",
        "\n",
        "    print(f\"Found {len(channels_data)} potential channels using example selectors.\")\n",
        "    return channels_data\n",
        "\n",
        "def scrape_profile_metadata(profile_url):\n",
        "    \"\"\"Attempts to scrape basic PUBLIC metadata from a profile URL (LinkedIn, Social).\"\"\"\n",
        "    metadata = {\"title\": None, \"bio_snippet\": None, \"url\": profile_url}\n",
        "    if not profile_url: return metadata\n",
        "    soup = get_soup(profile_url) # Uses retry logic\n",
        "    if not soup: return metadata # Return empty if fetch failed after retries\n",
        "\n",
        "    try:\n",
        "        # --- Using EXAMPLE selector - USER MUST VERIFY/UPDATE ---\n",
        "        title_tag = soup.select_one(LINKEDIN_TITLE_SELECTOR)\n",
        "        if title_tag: metadata[\"title\"] = clean_text(title_tag.get_text())\n",
        "\n",
        "        # --- Using EXAMPLE selectors - Try multiple patterns for Bio ---\n",
        "        bio = None\n",
        "        og_bio_tag = soup.select_one(SOCIAL_BIO_SELECTOR_OG)\n",
        "        if og_bio_tag and og_bio_tag.has_attr('content'): bio = clean_text(og_bio_tag['content'])\n",
        "        if not bio:\n",
        "            meta_bio_tag = soup.select_one(SOCIAL_BIO_SELECTOR_META)\n",
        "            if meta_bio_tag and meta_bio_tag.has_attr('content'): bio = clean_text(meta_bio_tag['content'])\n",
        "        metadata[\"bio_snippet\"] = bio\n",
        "\n",
        "    except Exception as e: print(f\"Error parsing metadata from {profile_url}: {e}\")\n",
        "    return metadata\n",
        "\n",
        "\n",
        "# --- Reddit Parsing Function ---\n",
        "def parse_reddit_findings(reddit_results):\n",
        "    \"\"\"Analyzes Reddit results for common themes/strategies.\"\"\"\n",
        "    notes = []\n",
        "    has_linkedin_mention = False; has_official_address_mention = False\n",
        "    for result in reddit_results:\n",
        "        content = (result.get(\"title\", \"\") + \" \" + result.get(\"snippet\", \"\")).lower()\n",
        "        if \"linkedin\" in content and (\"program director\" in content or \"music director\" in content or \"contact\" in content):\n",
        "            if not has_linkedin_mention: notes.append(\"Reddit users recommend using LinkedIn to find PDs/MDs.\"); has_linkedin_mention = True\n",
        "        if \"1221 avenue of the americas\" in content or \"official mail\" in content:\n",
        "             if not has_official_address_mention: notes.append(\"Reddit confirms official NYC mail-in address.\"); has_official_address_mention = True\n",
        "        if \"email format\" in content or re.search(r'\\b[a-z0-9._%+-]+@[a-z0-9.-]+\\.com\\b', content): # Broadened regex slightly\n",
        "            # Only add note once, avoid specific speculative formats\n",
        "             if \"speculation\" not in \" \".join(notes): notes.append(\"Reddit may contain *speculation* on email formats (treat with extreme caution).\")\n",
        "    return \" \".join(notes) if notes else \"No specific strategy insights found in Reddit search results.\"\n",
        "\n",
        "\n",
        "# --- Main Execution Logic ---\n",
        "def main():\n",
        "    \"\"\"Main function to orchestrate the scraping and data export.\"\"\"\n",
        "    all_data = []; processed_linkedin_urls = set(); processed_count = 0\n",
        "\n",
        "    # --- Load Targets ---\n",
        "    try:\n",
        "        print(f\"Loading targets from {INPUT_CSV_FILE}...\")\n",
        "        targets_df = pd.read_csv(INPUT_CSV_FILE); targets = targets_df.to_dict('records')\n",
        "        print(f\"Loaded {len(targets)} targets.\")\n",
        "    except FileNotFoundError: print(f\"Error: Input file '{INPUT_CSV_FILE}' not found.\"); return\n",
        "    except Exception as e: print(f\"Error reading input file {INPUT_CSV_FILE}: {e}\"); return\n",
        "\n",
        "    print(\"\\n--- Starting Processing ---\")\n",
        "    # --- Process Targets Loop ---\n",
        "    for target in targets:\n",
        "        target_name = target.get(\"Name\"); target_role_type = target.get(\"Role_Type\"); target_channel = target.get(\"Associated_Channel\")\n",
        "        if not target_role_type: print(f\"Skipping row with missing 'Role_Type': {target}\"); continue\n",
        "        print(f\"\\nProcessing Target: Name='{target_name}', Role='{target_role_type}', Channel='{target_channel}'\")\n",
        "\n",
        "        # --- Construct Search Query ---\n",
        "        query_parts = []\n",
        "        if target_name: query_parts.append(f'\"{target_name}\"')\n",
        "        # If Role_Type is 'Role', the Name column likely holds the actual role title\n",
        "        if target_role_type == \"Role\" and target_name: query_parts = [f'\"{target_name}\"']\n",
        "        # Don't add Role_Type if it was already used as the Name\n",
        "        elif target_role_type and target_role_type != \"Role\": query_parts.append(f'\"{target_role_type}\"')\n",
        "        query_parts.append(\"SiriusXM\")\n",
        "        if target_channel: query_parts.append(f'\"{target_channel}\"')\n",
        "        search_query = \" \".join(query_parts)\n",
        "\n",
        "        # --- Search Engine Lookup ---\n",
        "        found_results = search_engine_lookup(search_query)\n",
        "\n",
        "        # --- Process Found URLs ---\n",
        "        linkedin_url = None; twitter_url = None; instagram_url = None; other_urls = []\n",
        "        for result_item in found_results:\n",
        "            url = result_item.get('link')\n",
        "            if not url: continue # Skip if no URL in this result item\n",
        "\n",
        "            # Assign URLs (take first found for each type)\n",
        "            if (\"linkedin.com/in/\" in url or \"linkedin.com/pub/\" in url) and not linkedin_url: linkedin_url = url\n",
        "            elif \"twitter.com/\" in url and not twitter_url: twitter_url = url\n",
        "            elif \"instagram.com/\" in url and not instagram_url: instagram_url = url\n",
        "            elif any(domain in url for domain in [\"facebook.com/\", \"tiktok.com/\", \"siriusxm.com/hosts/\"]): other_urls.append(url)\n",
        "\n",
        "        # --- Deduplication Check ---\n",
        "        if linkedin_url and linkedin_url in processed_linkedin_urls:\n",
        "            print(f\"Skipping duplicate LinkedIn profile: {linkedin_url}\")\n",
        "            continue\n",
        "\n",
        "        # --- Prepare Data Entry ---\n",
        "        entry = {\n",
        "            \"Name\": target_name, \"Role_Type\": target_role_type, \"Associated_Channel\": target_channel,\n",
        "            \"LinkedIn_URL\": linkedin_url, \"LinkedIn_Title\": None, \"Twitter_URL\": twitter_url,\n",
        "            \"Instagram_URL\": instagram_url, \"Other_Social_URL\": \"; \".join(other_urls) if other_urls else None,\n",
        "            \"Social_Bio_Snippet\": None, \"Reddit_Insights\": None, \"Notes\": \"Requires manual verification.\"\n",
        "        }\n",
        "\n",
        "        # --- Metadata Scraping ---\n",
        "        if linkedin_url:\n",
        "            linkedin_meta = scrape_profile_metadata(linkedin_url)\n",
        "            entry[\"LinkedIn_Title\"] = linkedin_meta.get(\"title\")\n",
        "            processed_linkedin_urls.add(linkedin_url) # Add only if scrape attempted\n",
        "            # Refine name based on LinkedIn title if input name was blank or generic role title\n",
        "            if (not entry[\"Name\"] or entry[\"Name\"].lower() in [\"program director\", \"music director\", \"host\", \"dj\", \"curator\"]) and entry[\"LinkedIn_Title\"]:\n",
        "                 title_parts = re.split(r'\\s*-\\s*|\\s*\\|\\s*', entry[\"LinkedIn_Title\"], 1)\n",
        "                 if title_parts and title_parts[0].strip(): entry[\"Name\"] = title_parts[0].strip() # Update name\n",
        "\n",
        "        social_bio = None\n",
        "        if twitter_url:\n",
        "             twitter_meta = scrape_profile_metadata(twitter_url); social_bio = twitter_meta.get(\"bio_snippet\")\n",
        "        if instagram_url and not social_bio:\n",
        "             ig_meta = scrape_profile_metadata(instagram_url); social_bio = ig_meta.get(\"bio_snippet\")\n",
        "        entry[\"Social_Bio_Snippet\"] = social_bio\n",
        "\n",
        "        # --- Reddit Context ---\n",
        "        reddit_search_query_parts = [\"SiriusXM\"]\n",
        "        if entry[\"Name\"]: reddit_search_query_parts.append(f'\"{entry[\"Name\"]}\"') # Use potentially updated name\n",
        "        if target_channel: reddit_search_query_parts.append(f'\"{target_channel}\"')\n",
        "        reddit_search_query_parts.append(\"contact OR email OR submit\")\n",
        "        reddit_search_results = search_reddit(\" \".join(reddit_search_query_parts))\n",
        "        entry[\"Reddit_Insights\"] = parse_reddit_findings(reddit_search_results)\n",
        "\n",
        "        # --- Store Entry & Update Count ---\n",
        "        all_data.append(entry)\n",
        "        processed_count += 1\n",
        "        print(f\"Processed entry #{processed_count} for '{entry['Name']}'\")\n",
        "\n",
        "        # --- Periodic Saving ---\n",
        "        if processed_count > 0 and processed_count % SAVE_INTERVAL == 0:\n",
        "            print(f\"\\n--- Saving progress ({processed_count} records processed) to {OUTPUT_CSV_FILE} ---\")\n",
        "            try:\n",
        "                # Create DataFrame from current data\n",
        "                temp_df = pd.DataFrame(all_data)\n",
        "                # Define columns explicitly for consistent saving\n",
        "                final_columns = [\"Name\", \"Role_Type\", \"Associated_Channel\", \"LinkedIn_URL\", \"LinkedIn_Title\", \"Twitter_URL\", \"Instagram_URL\", \"Other_Social_URL\", \"Social_Bio_Snippet\", \"Reddit_Insights\", \"Notes\"]\n",
        "                # Add any missing columns with None/NaN before saving\n",
        "                for col in final_columns:\n",
        "                    if col not in temp_df.columns: temp_df[col] = None\n",
        "                temp_df = temp_df[final_columns] # Reorder\n",
        "                temp_df.to_csv(OUTPUT_CSV_FILE, index=False, encoding='utf-8')\n",
        "                print(\"--- Progress saved successfully. ---\")\n",
        "            except Exception as e: print(f\"--- Error saving progress: {e} ---\")\n",
        "            print(\"-\" * 20) # Separator\n",
        "\n",
        "    # --- Final Export ---\n",
        "    print(\"\\n--- Processing Complete ---\")\n",
        "    if not all_data: print(\"No data collected.\"); return\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "    final_columns = [\"Name\", \"Role_Type\", \"Associated_Channel\", \"LinkedIn_URL\", \"LinkedIn_Title\", \"Twitter_URL\", \"Instagram_URL\", \"Other_Social_URL\", \"Social_Bio_Snippet\", \"Reddit_Insights\", \"Notes\"]\n",
        "    for col in final_columns:\n",
        "        if col not in df.columns: df[col] = None\n",
        "    df = df[final_columns]\n",
        "    try:\n",
        "        df.to_csv(OUTPUT_CSV_FILE, index=False, encoding='utf-8')\n",
        "        print(f\"Final data exported successfully to {OUTPUT_CSV_FILE} ({len(df)} records)\")\n",
        "    except Exception as e: print(f\"Error exporting final data to CSV: {e}\")\n",
        "\n",
        "# --- Run the Script ---\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}