{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jv0EAcVinQ-0"
      },
      "outputs": [],
      "source": [
        "# Step 2: Install Necessary Libraries (Run this cell first in Colab)\n",
        "!pip install requests beautifulsoup4 pandas lxml --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://raw.githubusercontent.com/froggeric/Scrap-SiriusXM/refs/heads/main/targets.csv\" -O targets.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXqYnAn62K0D",
        "outputId": "3e52f59d-ae72-4aa7-b48f-02bc8a4aa8e3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-27 13:45:47--  https://raw.githubusercontent.com/froggeric/Scrap-SiriusXM/refs/heads/main/targets.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1385 (1.4K) [text/plain]\n",
            "Saving to: ‘targets.csv’\n",
            "\n",
            "\rtargets.csv           0%[                    ]       0  --.-KB/s               \rtargets.csv         100%[===================>]   1.35K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-03-27 13:45:47 (17.7 MB/s) - ‘targets.csv’ saved [1385/1385]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEOo-yPGnQ-1",
        "outputId": "27219e99-a7a1-47fb-fe1f-f21d5fd63e2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading targets from targets.csv...\n",
            "Loaded 44 targets.\n",
            "\n",
            "--- Starting Processing ---\n",
            "\n",
            "Processing Target: Name='Rocio Guerrero', Role='Role', Channel='Latin Music'\n",
            "Executing Google API search for: '\"Rocio Guerrero\" SiriusXM \"Latin Music\"' (Attempt 1/3)\n",
            "Found 2 results via Google API for '\"Rocio Guerrero\" SiriusXM \"Latin Music\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Rocio Guerrero\" \"Latin Music\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Rocio Guerrero\" \"Latin Music\" contact OR email OR submit'.\n",
            "Processed entry #1 for 'Rocio Guerrero'\n",
            "\n",
            "Processing Target: Name='Trinity Colón', Role='Role', Channel='Latin Pop/Urban'\n",
            "Executing Google API search for: '\"Trinity Colón\" SiriusXM \"Latin Pop/Urban\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Trinity Colón\" SiriusXM \"Latin Pop/Urban\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Trinity Colón\" \"Latin Pop/Urban\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Trinity Colón\" \"Latin Pop/Urban\" contact OR email OR submit'.\n",
            "Processed entry #2 for 'Trinity Colón'\n",
            "\n",
            "Processing Target: Name='Al Skop', Role='Role', Channel='The Highway'\n",
            "Executing Google API search for: '\"Al Skop\" SiriusXM \"The Highway\"' (Attempt 1/3)\n",
            "Found 5 results via Google API for '\"Al Skop\" SiriusXM \"The Highway\"'.\n",
            "Fetching: https://www.linkedin.com/in/alskop (Attempt 1/3)\n",
            "Warning: Network error fetching https://www.linkedin.com/in/alskop: 429 Client Error: Request denied for url: https://www.linkedin.com/in/alskop\n",
            "Retrying in 5 seconds...\n",
            "Fetching: https://www.linkedin.com/in/alskop (Attempt 2/3)\n",
            "Fetching: https://www.instagram.com/alskop/?hl=en (Attempt 1/3)\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Al Skop\" \"The Highway\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 1 results via Google API for 'site:reddit.com SiriusXM \"Al Skop\" \"The Highway\" contact OR email OR submit'.\n",
            "Processed entry #3 for 'Al Skop'\n",
            "\n",
            "Processing Target: Name='Jeff Regan', Role='Role', Channel='Alt Nation'\n",
            "Executing Google API search for: '\"Jeff Regan\" SiriusXM \"Alt Nation\"' (Attempt 1/3)\n",
            "Found 5 results via Google API for '\"Jeff Regan\" SiriusXM \"Alt Nation\"'.\n",
            "Fetching: https://www.linkedin.com/in/jeff-regan-0b6a406 (Attempt 1/3)\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Jeff Regan\" \"Alt Nation\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Jeff Regan\" \"Alt Nation\" contact OR email OR submit'.\n",
            "Processed entry #4 for 'Jeff Regan'\n",
            "\n",
            "Processing Target: Name='Geronimo', Role='Role', Channel='BPM / Dance'\n",
            "Executing Google API search for: '\"Geronimo\" SiriusXM \"BPM / Dance\"' (Attempt 1/3)\n",
            "Found 1 results via Google API for '\"Geronimo\" SiriusXM \"BPM / Dance\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Geronimo\" \"BPM / Dance\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Geronimo\" \"BPM / Dance\" contact OR email OR submit'.\n",
            "Processed entry #5 for 'Geronimo'\n",
            "\n",
            "Processing Target: Name='Kirsten', Role='Role', Channel='Radio Margaritaville'\n",
            "Executing Google API search for: '\"Kirsten\" SiriusXM \"Radio Margaritaville\"' (Attempt 1/3)\n",
            "Found 5 results via Google API for '\"Kirsten\" SiriusXM \"Radio Margaritaville\"'.\n",
            "Fetching: https://www.instagram.com/radiokirsten/?hl=en (Attempt 1/3)\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Kirsten\" \"Radio Margaritaville\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 1 results via Google API for 'site:reddit.com SiriusXM \"Kirsten\" \"Radio Margaritaville\" contact OR email OR submit'.\n",
            "Processed entry #6 for 'Kirsten'\n",
            "\n",
            "Processing Target: Name='Chris Muckley', Role='Role', Channel='SiriusXMU'\n",
            "Executing Google API search for: '\"Chris Muckley\" SiriusXM \"SiriusXMU\"' (Attempt 1/3)\n",
            "Found 5 results via Google API for '\"Chris Muckley\" SiriusXM \"SiriusXMU\"'.\n",
            "Fetching: https://www.linkedin.com/in/cmuckley (Attempt 1/3)\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Chris Muckley\" \"SiriusXMU\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Chris Muckley\" \"SiriusXMU\" contact OR email OR submit'.\n",
            "Processed entry #7 for 'Chris Muckley'\n",
            "\n",
            "Processing Target: Name='Ron Mills', Role='Role', Channel='HipHop/R&B'\n",
            "Executing Google API search for: '\"Ron Mills\" SiriusXM \"HipHop/R&B\"' (Attempt 1/3)\n",
            "Found 2 results via Google API for '\"Ron Mills\" SiriusXM \"HipHop/R&B\"'.\n",
            "Fetching: https://www.instagram.com/prettylou11/?api=Instagram%E7%BE%A4%E5%8F%91%E5%B7%A5%E5%85%B7%F0%9F%A7%A7-[%E8%AE%A4%E5%87%86%E5%A4%A7%E8%BD%A9TG%3A%40TC2397431747]-IG%E7%AD%9B%E9%80%89%E8%BD%AF%E4%BB%B6%2FIG%E7%BE%A4%E5%8F%91%E5%8D%8F%E8%AE%AE%2FIG%E6%8B%89%E7%BE%A4%E8%BD%AF%E4%BB%B6.zuqy&hl=zh-cn (Attempt 1/3)\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Ron Mills\" \"HipHop/R&B\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Ron Mills\" \"HipHop/R&B\" contact OR email OR submit'.\n",
            "Processed entry #8 for 'Ron Mills'\n",
            "\n",
            "Processing Target: Name='B.J. Stone', Role='Role', Channel='Pop Hits'\n",
            "Executing Google API search for: '\"B.J. Stone\" SiriusXM \"Pop Hits\"' (Attempt 1/3)\n",
            "Found 2 results via Google API for '\"B.J. Stone\" SiriusXM \"Pop Hits\"'.\n",
            "Fetching: https://www.linkedin.com/in/bj-stone-72a11314 (Attempt 1/3)\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"B.J. Stone\" \"Pop Hits\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"B.J. Stone\" \"Pop Hits\" contact OR email OR submit'.\n",
            "Processed entry #9 for 'B.J. Stone'\n",
            "\n",
            "Processing Target: Name='Ismael Santa Cruz', Role='Role', Channel='Latin Music'\n",
            "Executing Google API search for: '\"Ismael Santa Cruz\" SiriusXM \"Latin Music\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Ismael Santa Cruz\" SiriusXM \"Latin Music\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Ismael Santa Cruz\" \"Latin Music\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Ismael Santa Cruz\" \"Latin Music\" contact OR email OR submit'.\n",
            "Processed entry #10 for 'Ismael Santa Cruz'\n",
            "\n",
            "Processing Target: Name='Kid Leo', Role='Role', Channel='Underground Garage'\n",
            "Executing Google API search for: '\"Kid Leo\" SiriusXM \"Underground Garage\"' (Attempt 1/3)\n",
            "Found 5 results via Google API for '\"Kid Leo\" SiriusXM \"Underground Garage\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Kid Leo\" \"Underground Garage\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 3 results via Google API for 'site:reddit.com SiriusXM \"Kid Leo\" \"Underground Garage\" contact OR email OR submit'.\n",
            "Processed entry #11 for 'Kid Leo'\n",
            "\n",
            "Processing Target: Name='Music Director', Role='Role', Channel='Águila'\n",
            "Executing Google API search for: '\"Music Director\" SiriusXM \"Águila\"' (Attempt 1/3)\n",
            "Found 5 results via Google API for '\"Music Director\" SiriusXM \"Águila\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Music Director\" \"Águila\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Music Director\" \"Águila\" contact OR email OR submit'.\n",
            "Processed entry #12 for 'Music Director'\n",
            "\n",
            "Processing Target: Name='Music Director', Role='Role', Channel='Caliente'\n",
            "Executing Google API search for: '\"Music Director\" SiriusXM \"Caliente\"' (Attempt 1/3)\n",
            "Found 5 results via Google API for '\"Music Director\" SiriusXM \"Caliente\"'.\n",
            "Fetching: https://www.linkedin.com/in/chiefraymondhernandez (Attempt 1/3)\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Music Director\" \"Caliente\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Music Director\" \"Caliente\" contact OR email OR submit'.\n",
            "Processed entry #13 for 'Music Director'\n",
            "\n",
            "Processing Target: Name='Program Director', Role='Role', Channel='Hip Hop Nation'\n",
            "Executing Google API search for: '\"Program Director\" SiriusXM \"Hip Hop Nation\"' (Attempt 1/3)\n",
            "Found 5 results via Google API for '\"Program Director\" SiriusXM \"Hip Hop Nation\"'.\n",
            "Fetching: https://www.linkedin.com/in/reginald-hawkins (Attempt 1/3)\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Program Director\" \"Hip Hop Nation\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Program Director\" \"Hip Hop Nation\" contact OR email OR submit'.\n",
            "Processed entry #14 for 'Program Director'\n",
            "\n",
            "Processing Target: Name='Music Director', Role='Role', Channel='Octane'\n",
            "Executing Google API search for: '\"Music Director\" SiriusXM \"Octane\"' (Attempt 1/3)\n",
            "Found 5 results via Google API for '\"Music Director\" SiriusXM \"Octane\"'.\n",
            "Fetching: https://www.linkedin.com/in/vincent-rockwell-usuriello-34910915 (Attempt 1/3)\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Music Director\" \"Octane\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 1 results via Google API for 'site:reddit.com SiriusXM \"Music Director\" \"Octane\" contact OR email OR submit'.\n",
            "Processed entry #15 for 'Music Director'\n",
            "\n",
            "Processing Target: Name='Program Director', Role='Role', Channel='Pitbull's Globalization'\n",
            "Executing Google API search for: '\"Program Director\" SiriusXM \"Pitbull's Globalization\"' (Attempt 1/3)\n",
            "Found 5 results via Google API for '\"Program Director\" SiriusXM \"Pitbull's Globalization\"'.\n",
            "Fetching: https://www.linkedin.com/in/edwinphenom (Attempt 1/3)\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Program Director\" \"Pitbull's Globalization\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Program Director\" \"Pitbull's Globalization\" contact OR email OR submit'.\n",
            "Processed entry #16 for 'Program Director'\n",
            "\n",
            "Processing Target: Name='Program Director', Role='Role', Channel='Shaggy's Boombastic Radio'\n",
            "Executing Google API search for: '\"Program Director\" SiriusXM \"Shaggy's Boombastic Radio\"' (Attempt 1/3)\n",
            "Found 3 results via Google API for '\"Program Director\" SiriusXM \"Shaggy's Boombastic Radio\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Program Director\" \"Shaggy's Boombastic Radio\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Program Director\" \"Shaggy's Boombastic Radio\" contact OR email OR submit'.\n",
            "Processed entry #17 for 'Program Director'\n",
            "\n",
            "Processing Target: Name='Program Director', Role='Role', Channel='Milagro Radio'\n",
            "Executing Google API search for: '\"Program Director\" SiriusXM \"Milagro Radio\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Program Director\" SiriusXM \"Milagro Radio\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Program Director\" \"Milagro Radio\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Program Director\" \"Milagro Radio\" contact OR email OR submit'.\n",
            "Processed entry #18 for 'Program Director'\n",
            "\n",
            "Processing Target: Name='Cody Alan', Role='DJ/Host', Channel='The Highway'\n",
            "Executing Google API search for: '\"Cody Alan\" \"DJ/Host\" SiriusXM \"The Highway\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Cody Alan\" \"DJ/Host\" SiriusXM \"The Highway\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Cody Alan\" \"The Highway\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Cody Alan\" \"The Highway\" contact OR email OR submit'.\n",
            "Processed entry #19 for 'Cody Alan'\n",
            "\n",
            "Processing Target: Name='Buzz Brainard', Role='DJ/Host', Channel='The Highway'\n",
            "Executing Google API search for: '\"Buzz Brainard\" \"DJ/Host\" SiriusXM \"The Highway\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Buzz Brainard\" \"DJ/Host\" SiriusXM \"The Highway\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Buzz Brainard\" \"The Highway\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 2 results via Google API for 'site:reddit.com SiriusXM \"Buzz Brainard\" \"The Highway\" contact OR email OR submit'.\n",
            "Processed entry #20 for 'Buzz Brainard'\n",
            "\n",
            "Processing Target: Name='Ania Hammar', Role='DJ/Host', Channel='The Highway'\n",
            "Executing Google API search for: '\"Ania Hammar\" \"DJ/Host\" SiriusXM \"The Highway\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Ania Hammar\" \"DJ/Host\" SiriusXM \"The Highway\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Ania Hammar\" \"The Highway\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Ania Hammar\" \"The Highway\" contact OR email OR submit'.\n",
            "Processed entry #21 for 'Ania Hammar'\n",
            "\n",
            "Processing Target: Name='Torae', Role='DJ/Host', Channel='Hip Hop Nation'\n",
            "Executing Google API search for: '\"Torae\" \"DJ/Host\" SiriusXM \"Hip Hop Nation\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Torae\" \"DJ/Host\" SiriusXM \"Hip Hop Nation\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Torae\" \"Hip Hop Nation\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Torae\" \"Hip Hop Nation\" contact OR email OR submit'.\n",
            "Processed entry #22 for 'Torae'\n",
            "\n",
            "Processing Target: Name='Madison', Role='DJ/Host', Channel='Alt Nation'\n",
            "Executing Google API search for: '\"Madison\" \"DJ/Host\" SiriusXM \"Alt Nation\"' (Attempt 1/3)\n",
            "Found 2 results via Google API for '\"Madison\" \"DJ/Host\" SiriusXM \"Alt Nation\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Madison\" \"Alt Nation\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 3 results via Google API for 'site:reddit.com SiriusXM \"Madison\" \"Alt Nation\" contact OR email OR submit'.\n",
            "Processed entry #23 for 'Madison'\n",
            "\n",
            "Processing Target: Name='Jose Mangin', Role='DJ/Host', Channel='Octane'\n",
            "Executing Google API search for: '\"Jose Mangin\" \"DJ/Host\" SiriusXM \"Octane\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Jose Mangin\" \"DJ/Host\" SiriusXM \"Octane\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Jose Mangin\" \"Octane\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 3 results via Google API for 'site:reddit.com SiriusXM \"Jose Mangin\" \"Octane\" contact OR email OR submit'.\n",
            "Processed entry #24 for 'Jose Mangin'\n",
            "\n",
            "Processing Target: Name='Kayla Riley', Role='DJ/Host', Channel='Octane'\n",
            "Executing Google API search for: '\"Kayla Riley\" \"DJ/Host\" SiriusXM \"Octane\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Kayla Riley\" \"DJ/Host\" SiriusXM \"Octane\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Kayla Riley\" \"Octane\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Kayla Riley\" \"Octane\" contact OR email OR submit'.\n",
            "Processed entry #25 for 'Kayla Riley'\n",
            "\n",
            "--- Saving progress (25 records processed) to siriusxm_contacts_output.csv ---\n",
            "--- Progress saved successfully. ---\n",
            "--------------------\n",
            "\n",
            "Processing Target: Name='Caity Babs', Role='DJ/Host', Channel='Octane'\n",
            "Executing Google API search for: '\"Caity Babs\" \"DJ/Host\" SiriusXM \"Octane\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Caity Babs\" \"DJ/Host\" SiriusXM \"Octane\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Caity Babs\" \"Octane\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 3 results via Google API for 'site:reddit.com SiriusXM \"Caity Babs\" \"Octane\" contact OR email OR submit'.\n",
            "Processed entry #26 for 'Caity Babs'\n",
            "\n",
            "Processing Target: Name='Shannon Gunz', Role='DJ/Host', Channel='Octane'\n",
            "Executing Google API search for: '\"Shannon Gunz\" \"DJ/Host\" SiriusXM \"Octane\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Shannon Gunz\" \"DJ/Host\" SiriusXM \"Octane\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Shannon Gunz\" \"Octane\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 2 results via Google API for 'site:reddit.com SiriusXM \"Shannon Gunz\" \"Octane\" contact OR email OR submit'.\n",
            "Processed entry #27 for 'Shannon Gunz'\n",
            "\n",
            "Processing Target: Name='Richard Blade', Role='DJ/Host', Channel='1st Wave'\n",
            "Executing Google API search for: '\"Richard Blade\" \"DJ/Host\" SiriusXM \"1st Wave\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Richard Blade\" \"DJ/Host\" SiriusXM \"1st Wave\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Richard Blade\" \"1st Wave\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 3 results via Google API for 'site:reddit.com SiriusXM \"Richard Blade\" \"1st Wave\" contact OR email OR submit'.\n",
            "Processed entry #28 for 'Richard Blade'\n",
            "\n",
            "Processing Target: Name='Liquid Todd', Role='DJ/Host', Channel='BPM'\n",
            "Executing Google API search for: '\"Liquid Todd\" \"DJ/Host\" SiriusXM \"BPM\"' (Attempt 1/3)\n",
            "Found 2 results via Google API for '\"Liquid Todd\" \"DJ/Host\" SiriusXM \"BPM\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Liquid Todd\" \"BPM\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Liquid Todd\" \"BPM\" contact OR email OR submit'.\n",
            "Processed entry #29 for 'Liquid Todd'\n",
            "\n",
            "Processing Target: Name='Jenny Eliscu', Role='DJ/Host', Channel='SiriusXMU'\n",
            "Executing Google API search for: '\"Jenny Eliscu\" \"DJ/Host\" SiriusXM \"SiriusXMU\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Jenny Eliscu\" \"DJ/Host\" SiriusXM \"SiriusXMU\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Jenny Eliscu\" \"SiriusXMU\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 2 results via Google API for 'site:reddit.com SiriusXM \"Jenny Eliscu\" \"SiriusXMU\" contact OR email OR submit'.\n",
            "Processed entry #30 for 'Jenny Eliscu'\n",
            "\n",
            "Processing Target: Name='Earle Bailey', Role='DJ/Host', Channel='Deep Tracks'\n",
            "Executing Google API search for: '\"Earle Bailey\" \"DJ/Host\" SiriusXM \"Deep Tracks\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Earle Bailey\" \"DJ/Host\" SiriusXM \"Deep Tracks\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Earle Bailey\" \"Deep Tracks\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 3 results via Google API for 'site:reddit.com SiriusXM \"Earle Bailey\" \"Deep Tracks\" contact OR email OR submit'.\n",
            "Processed entry #31 for 'Earle Bailey'\n",
            "\n",
            "Processing Target: Name='Meg Griffin', Role='DJ/Host', Channel='Deep Tracks'\n",
            "Executing Google API search for: '\"Meg Griffin\" \"DJ/Host\" SiriusXM \"Deep Tracks\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Meg Griffin\" \"DJ/Host\" SiriusXM \"Deep Tracks\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Meg Griffin\" \"Deep Tracks\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 3 results via Google API for 'site:reddit.com SiriusXM \"Meg Griffin\" \"Deep Tracks\" contact OR email OR submit'.\n",
            "Processed entry #32 for 'Meg Griffin'\n",
            "\n",
            "Processing Target: Name='Cayman Kelly', Role='DJ/Host', Channel='FLY'\n",
            "Executing Google API search for: '\"Cayman Kelly\" \"DJ/Host\" SiriusXM \"FLY\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Cayman Kelly\" \"DJ/Host\" SiriusXM \"FLY\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Cayman Kelly\" \"FLY\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Cayman Kelly\" \"FLY\" contact OR email OR submit'.\n",
            "Processed entry #33 for 'Cayman Kelly'\n",
            "\n",
            "Processing Target: Name='Heather B.', Role='DJ/Host', Channel='FLY'\n",
            "Executing Google API search for: '\"Heather B.\" \"DJ/Host\" SiriusXM \"FLY\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Heather B.\" \"DJ/Host\" SiriusXM \"FLY\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Heather B.\" \"FLY\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 1 results via Google API for 'site:reddit.com SiriusXM \"Heather B.\" \"FLY\" contact OR email OR submit'.\n",
            "Processed entry #34 for 'Heather B.'\n",
            "\n",
            "Processing Target: Name='Marisol El Bombón', Role='DJ/Host', Channel='Caliente'\n",
            "Executing Google API search for: '\"Marisol El Bombón\" \"DJ/Host\" SiriusXM \"Caliente\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Marisol El Bombón\" \"DJ/Host\" SiriusXM \"Caliente\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Marisol El Bombón\" \"Caliente\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Marisol El Bombón\" \"Caliente\" contact OR email OR submit'.\n",
            "Processed entry #35 for 'Marisol El Bombón'\n",
            "\n",
            "Processing Target: Name='AJ El Kallejero', Role='DJ/Host', Channel='Flow Nación'\n",
            "Executing Google API search for: '\"AJ El Kallejero\" \"DJ/Host\" SiriusXM \"Flow Nación\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"AJ El Kallejero\" \"DJ/Host\" SiriusXM \"Flow Nación\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"AJ El Kallejero\" \"Flow Nación\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"AJ El Kallejero\" \"Flow Nación\" contact OR email OR submit'.\n",
            "Processed entry #36 for 'AJ El Kallejero'\n",
            "\n",
            "Processing Target: Name='Stefi Chacon', Role='DJ/Host', Channel='Viva'\n",
            "Executing Google API search for: '\"Stefi Chacon\" \"DJ/Host\" SiriusXM \"Viva\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Stefi Chacon\" \"DJ/Host\" SiriusXM \"Viva\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Stefi Chacon\" \"Viva\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Stefi Chacon\" \"Viva\" contact OR email OR submit'.\n",
            "Processed entry #37 for 'Stefi Chacon'\n",
            "\n",
            "Processing Target: Name='Israel Salazar', Role='DJ/Host', Channel='Águila'\n",
            "Executing Google API search for: '\"Israel Salazar\" \"DJ/Host\" SiriusXM \"Águila\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Israel Salazar\" \"DJ/Host\" SiriusXM \"Águila\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Israel Salazar\" \"Águila\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Israel Salazar\" \"Águila\" contact OR email OR submit'.\n",
            "Processed entry #38 for 'Israel Salazar'\n",
            "\n",
            "Processing Target: Name='Bryant Pino', Role='DJ/Host', Channel='Águila'\n",
            "Executing Google API search for: '\"Bryant Pino\" \"DJ/Host\" SiriusXM \"Águila\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Bryant Pino\" \"DJ/Host\" SiriusXM \"Águila\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Bryant Pino\" \"Águila\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Bryant Pino\" \"Águila\" contact OR email OR submit'.\n",
            "Processed entry #39 for 'Bryant Pino'\n",
            "\n",
            "Processing Target: Name='Diane Fong', Role='DJ/Host', Channel='Latidos'\n",
            "Executing Google API search for: '\"Diane Fong\" \"DJ/Host\" SiriusXM \"Latidos\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Diane Fong\" \"DJ/Host\" SiriusXM \"Latidos\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Diane Fong\" \"Latidos\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Diane Fong\" \"Latidos\" contact OR email OR submit'.\n",
            "Processed entry #40 for 'Diane Fong'\n",
            "\n",
            "Processing Target: Name='DJ Kito', Role='DJ/Host', Channel='Caliente'\n",
            "Executing Google API search for: '\"DJ Kito\" \"DJ/Host\" SiriusXM \"Caliente\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"DJ Kito\" \"DJ/Host\" SiriusXM \"Caliente\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"DJ Kito\" \"Caliente\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"DJ Kito\" \"Caliente\" contact OR email OR submit'.\n",
            "Processed entry #41 for 'DJ Kito'\n",
            "\n",
            "Processing Target: Name='Sandra Peña', Role='DJ/Host', Channel='Caliente'\n",
            "Executing Google API search for: '\"Sandra Peña\" \"DJ/Host\" SiriusXM \"Caliente\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Sandra Peña\" \"DJ/Host\" SiriusXM \"Caliente\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Sandra Peña\" \"Caliente\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Sandra Peña\" \"Caliente\" contact OR email OR submit'.\n",
            "Processed entry #42 for 'Sandra Peña'\n",
            "\n",
            "Processing Target: Name='DJ C-Riz', Role='DJ/Host', Channel='Flow Nación'\n",
            "Executing Google API search for: '\"DJ C-Riz\" \"DJ/Host\" SiriusXM \"Flow Nación\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"DJ C-Riz\" \"DJ/Host\" SiriusXM \"Flow Nación\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"DJ C-Riz\" \"Flow Nación\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"DJ C-Riz\" \"Flow Nación\" contact OR email OR submit'.\n",
            "Processed entry #43 for 'DJ C-Riz'\n",
            "\n",
            "Processing Target: Name='Marco Juárez', Role='Role', Channel='Pandora Regional Mexican'\n",
            "Executing Google API search for: '\"Marco Juárez\" SiriusXM \"Pandora Regional Mexican\"' (Attempt 1/3)\n",
            "Found 0 results via Google API for '\"Marco Juárez\" SiriusXM \"Pandora Regional Mexican\"'.\n",
            "Executing Google API search for: 'site:reddit.com SiriusXM \"Marco Juárez\" \"Pandora Regional Mexican\" contact OR email OR submit' (Attempt 1/3)\n",
            "Found 0 results via Google API for 'site:reddit.com SiriusXM \"Marco Juárez\" \"Pandora Regional Mexican\" contact OR email OR submit'.\n",
            "Processed entry #44 for 'Marco Juárez'\n",
            "\n",
            "--- Processing Complete ---\n",
            "Final data exported successfully to siriusxm_contacts_output.csv (44 records)\n"
          ]
        }
      ],
      "source": [
        "# --- Imports ---\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import urllib.parse\n",
        "import json\n",
        "import os\n",
        "try:\n",
        "    # To use Colab Secrets for API Keys\n",
        "    from google.colab import userdata\n",
        "    SECRETS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SECRETS_AVAILABLE = False\n",
        "    # print(\"Warning: 'google.colab.userdata' not available...\") # Keep silent unless debugging\n",
        "\n",
        "# ==============================================================================\n",
        "# === USER CONFIGURATION AREA ==================================================\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Input / Output Files ---\n",
        "# USER ACTION: Create this CSV file and upload to Colab or specify path.\n",
        "# Expected columns: 'Name' (optional), 'Role_Type' (e.g., \"DJ/Host\", \"Role\"), 'Associated_Channel' (optional)\n",
        "INPUT_CSV_FILE = \"targets.csv\"\n",
        "OUTPUT_CSV_FILE = \"siriusxm_contacts_output.csv\" # Output filename\n",
        "\n",
        "# --- Scraping Behavior ---\n",
        "REQUEST_DELAY = 2       # Seconds delay between direct web requests (be polite!)\n",
        "REQUEST_TIMEOUT = 15    # Seconds before timeout\n",
        "MAX_RETRIES = 2         # Number of retries after the initial attempt for network errors\n",
        "RETRY_DELAY = 5         # Seconds to wait before retrying a failed request\n",
        "SAVE_INTERVAL = 25      # Save progress to CSV every N records processed\n",
        "\n",
        "# --- API Keys / CSE ID ---\n",
        "# USER ACTION: Store these in Colab Secrets (Key icon in left sidebar)\n",
        "# Recommended Secret Names: GOOGLE_API_KEY, GOOGLE_CSE_ID\n",
        "\n",
        "# --- Web Request Headers ---\n",
        "HEADERS = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
        "    'Accept-Language': 'en-US,en;q=0.9',\n",
        "}\n",
        "\n",
        "# --- Google API ---\n",
        "GOOGLE_SEARCH_API_URL = \"https://www.googleapis.com/customsearch/v1\"\n",
        "\n",
        "# --- CSS Selectors (USER MUST VERIFY/UPDATE THESE by inspecting live websites!) ---\n",
        "# These are plausible examples based on common structures but WILL likely need changes.\n",
        "\n",
        "# SiriusXM Channel Page Selectors (Example - Verify!)\n",
        "SIRIUSXM_CHANNELS_URL = \"https://www.siriusxm.com/channels\"\n",
        "SIRIUSXM_CHANNEL_LIST_SELECTOR = \"a[href*='/channels/']\" # Try finding links pointing to specific channels\n",
        "SIRIUSXM_CHANNEL_NAME_SELECTOR = \"h4, div[role='heading']\" # Try common heading tags or divs marked as headings\n",
        "SIRIUSXM_CHANNEL_URL_SELECTOR = None # Set to None if the LIST_SELECTOR is the 'a' tag itself.\n",
        "\n",
        "# LinkedIn Public Profile Selectors (Example - Verify!)\n",
        "LINKEDIN_TITLE_SELECTOR = \"title\" # Usually reliable\n",
        "\n",
        "# Social Media Profile Selectors (Example - Verify!)\n",
        "SOCIAL_BIO_SELECTOR_OG = \"meta[property='og:description']\" # Common, try first\n",
        "SOCIAL_BIO_SELECTOR_META = \"meta[name='description']\"      # Fallback\n",
        "\n",
        "# Follower counts - Static selectors usually FAIL. Requires JS/Selenium/API.\n",
        "SOCIAL_FOLLOWER_SELECTOR = None # Explicitly None as requests/BS4 cannot reliably get this\n",
        "\n",
        "# ==============================================================================\n",
        "# === END USER CONFIGURATION AREA ==============================================\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def get_soup(url):\n",
        "    \"\"\"\n",
        "    Fetches a URL and returns a BeautifulSoup object, with error handling and retries.\n",
        "    Retries on Timeout, ConnectionError, and general RequestException.\n",
        "    \"\"\"\n",
        "    soup = None\n",
        "    for attempt in range(MAX_RETRIES + 1):\n",
        "        try:\n",
        "            print(f\"Fetching: {url} (Attempt {attempt + 1}/{MAX_RETRIES + 1})\")\n",
        "            response = requests.get(url, headers=HEADERS, timeout=REQUEST_TIMEOUT)\n",
        "            response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "            soup = BeautifulSoup(response.content, 'lxml')\n",
        "            # print(f\"Successfully fetched: {url}\") # Reduce noise\n",
        "            break # Exit loop on success\n",
        "\n",
        "        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError, requests.exceptions.RequestException) as e:\n",
        "            print(f\"Warning: Network error fetching {url}: {e}\")\n",
        "            if attempt < MAX_RETRIES:\n",
        "                print(f\"Retrying in {RETRY_DELAY} seconds...\")\n",
        "                time.sleep(RETRY_DELAY)\n",
        "                continue # <<<--- CORRECTED: Added continue to proceed to next attempt\n",
        "            else:\n",
        "                print(f\"Error: Max retries reached for {url}. Giving up.\")\n",
        "                soup = None # Ensure None on final failure\n",
        "\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "             print(f\"Error: HTTP Error {e.response.status_code} fetching {url} (Not retrying).\")\n",
        "             soup = None # Ensure None on non-retryable HTTP error\n",
        "             break # Exit loop\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: Could not process {url}: {e} (Not retrying).\")\n",
        "            soup = None # Ensure None on other errors\n",
        "            break # Exit loop\n",
        "\n",
        "    # Delay slightly even if successful, more if failed? No, keep consistent delay after attempts.\n",
        "    time.sleep(REQUEST_DELAY / 2) # Slightly shorter delay after direct fetches ok\n",
        "    return soup\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Helper to strip whitespace and potentially clean up text.\"\"\"\n",
        "    return text.strip() if text else None\n",
        "\n",
        "\n",
        "# --- Search Functions ---\n",
        "def search_engine_lookup(query, api_key=None, cse_id=None, num_results=5):\n",
        "    \"\"\"\n",
        "    Executes a web search using the Google Custom Search JSON API with retries.\n",
        "    \"\"\"\n",
        "    # --- Get API Key and CSE ID securely ---\n",
        "    if not api_key:\n",
        "        if SECRETS_AVAILABLE:\n",
        "            try: api_key = userdata.get('GOOGLE_API_KEY')\n",
        "            except userdata.SecretNotFoundError: print(\"Error: Google API Key not found in Colab Secrets ('GOOGLE_API_KEY').\"); return []\n",
        "        else:\n",
        "            api_key = os.environ.get('GOOGLE_API_KEY');\n",
        "            if not api_key: print(\"Error: GOOGLE_API_KEY not found in environment variables.\"); return []\n",
        "    if not cse_id:\n",
        "        if SECRETS_AVAILABLE:\n",
        "            try: cse_id = userdata.get('GOOGLE_CSE_ID')\n",
        "            except userdata.SecretNotFoundError: print(\"Error: Google CSE ID not found in Colab Secrets ('GOOGLE_CSE_ID').\"); return []\n",
        "        else:\n",
        "            cse_id = os.environ.get('GOOGLE_CSE_ID');\n",
        "            if not cse_id: print(\"Error: GOOGLE_CSE_ID not found in environment variables.\"); return []\n",
        "\n",
        "    params = {'key': api_key, 'cx': cse_id, 'q': query, 'num': num_results}\n",
        "    results_found = []\n",
        "    response_json = None\n",
        "\n",
        "    for attempt in range(MAX_RETRIES + 1):\n",
        "        try:\n",
        "            print(f\"Executing Google API search for: '{query}' (Attempt {attempt + 1}/{MAX_RETRIES + 1})\")\n",
        "            response = requests.get(GOOGLE_SEARCH_API_URL, params=params, headers=HEADERS, timeout=REQUEST_TIMEOUT)\n",
        "            response.raise_for_status()\n",
        "            response_json = response.json() # Try decoding JSON here\n",
        "            # print(f\"Successfully got API response for: '{query}'\") # Reduce noise\n",
        "            break # Exit loop on success\n",
        "\n",
        "        except (requests.exceptions.Timeout, requests.exceptions.ConnectionError, requests.exceptions.RequestException) as e:\n",
        "            print(f\"Warning: Network error during Google API search for '{query}': {e}\")\n",
        "            if attempt < MAX_RETRIES:\n",
        "                print(f\"Retrying in {RETRY_DELAY} seconds...\")\n",
        "                time.sleep(RETRY_DELAY)\n",
        "                continue # <<<--- CORRECTED: Added continue to proceed to next attempt\n",
        "            else:\n",
        "                print(f\"Error: Max retries reached for Google API search '{query}'. Giving up.\")\n",
        "                return [] # Return empty on final failure\n",
        "\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "             print(f\"Error: HTTP Error {e.response.status_code} during Google API search for '{query}' (Not retrying). Status: {e.response.text}\")\n",
        "             return []\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "             print(f\"Error: Could not decode JSON response from Google API for '{query}' (Not retrying). Response text: {response.text[:200]}...\")\n",
        "             return []\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: An unexpected error occurred during Google API search for '{query}': {e} (Not retrying).\")\n",
        "            return []\n",
        "\n",
        "    # --- Parse Response ---\n",
        "    if response_json:\n",
        "        if 'error' in response_json:\n",
        "            error_details = response_json['error'].get('message', 'Unknown API error')\n",
        "            print(f\"Error: Google API returned an application-level error: {error_details}\")\n",
        "            return []\n",
        "        items = response_json.get('items', [])\n",
        "        # print(f\"No search results found via API for '{query}'.\") # Only print if needed\n",
        "        for item in items:\n",
        "            link = item.get('link'); title = item.get('title'); snippet = item.get('snippet')\n",
        "            if link: results_found.append({'link': link, 'title': title, 'snippet': snippet})\n",
        "        print(f\"Found {len(results_found)} results via Google API for '{query}'.\")\n",
        "    else:\n",
        "        print(f\"No valid response received after retries for Google API search '{query}'.\")\n",
        "        return []\n",
        "\n",
        "    time.sleep(max(REQUEST_DELAY / 4, 0.5)) # Short delay after successful API call\n",
        "    return results_found\n",
        "\n",
        "def search_reddit(query, api_key=None, cse_id=None, num_results=3):\n",
        "    \"\"\"Searches Reddit using the Google Custom Search API by adding 'site:reddit.com'.\"\"\"\n",
        "    reddit_query = f\"site:reddit.com {query}\"\n",
        "    reddit_results = search_engine_lookup(query=reddit_query, api_key=api_key, cse_id=cse_id, num_results=num_results)\n",
        "    return reddit_results\n",
        "\n",
        "\n",
        "# --- Scraping Functions ---\n",
        "def scrape_siriusxm_channels(url):\n",
        "    \"\"\"Scrapes the main SiriusXM channels page for channel names and URLs.\"\"\"\n",
        "    print(\"Scraping SiriusXM Channel List...\")\n",
        "    soup = get_soup(url) # Uses retry logic\n",
        "    channels_data = []\n",
        "    if not soup: print(\"Failed to get SiriusXM channel page soup after retries.\"); return channels_data\n",
        "\n",
        "    # --- Using EXAMPLE selector - USER MUST VERIFY/UPDATE ---\n",
        "    channel_link_elements = soup.select(SIRIUSXM_CHANNEL_LIST_SELECTOR)\n",
        "    if not channel_link_elements: print(f\"Warning: SiriusXM channel selector '{SIRIUSXM_CHANNEL_LIST_SELECTOR}' not found.\"); return channels_data\n",
        "\n",
        "    processed_urls = set()\n",
        "    for element in channel_link_elements:\n",
        "        try:\n",
        "            channel_url_relative = element.get('href')\n",
        "            if not channel_url_relative or not channel_url_relative.startswith('/channels/'): continue\n",
        "            channel_url_absolute = urllib.parse.urljoin(url, channel_url_relative)\n",
        "            if channel_url_absolute in processed_urls: continue\n",
        "            processed_urls.add(channel_url_absolute)\n",
        "\n",
        "            # --- Using EXAMPLE selectors - USER MUST VERIFY/UPDATE ---\n",
        "            name_tag = element.select_one(SIRIUSXM_CHANNEL_NAME_SELECTOR)\n",
        "            name = clean_text(name_tag.get_text()) if name_tag else None\n",
        "            if not name: name = clean_text(element.get('aria-label'))\n",
        "            if not name:\n",
        "                img_tag = element.find('img');\n",
        "                if img_tag: name = clean_text(img_tag.get('alt'))\n",
        "            if not name and channel_url_relative:\n",
        "                 name_guess = channel_url_relative.split('/')[-1]\n",
        "                 if name_guess: name = name_guess.replace('-', ' ').title()\n",
        "\n",
        "            if name: channels_data.append({\"channel_name\": name, \"channel_url\": channel_url_absolute})\n",
        "            else: print(f\"Warning: Could not determine name for channel URL: {channel_url_absolute}\")\n",
        "        except Exception as e: print(f\"Error parsing a channel element: {e}\")\n",
        "\n",
        "    print(f\"Found {len(channels_data)} potential channels using example selectors.\")\n",
        "    return channels_data\n",
        "\n",
        "def scrape_profile_metadata(profile_url):\n",
        "    \"\"\"Attempts to scrape basic PUBLIC metadata from a profile URL (LinkedIn, Social).\"\"\"\n",
        "    metadata = {\"title\": None, \"bio_snippet\": None, \"url\": profile_url}\n",
        "    if not profile_url: return metadata\n",
        "    soup = get_soup(profile_url) # Uses retry logic\n",
        "    if not soup: return metadata # Return empty if fetch failed after retries\n",
        "\n",
        "    try:\n",
        "        # --- Using EXAMPLE selector - USER MUST VERIFY/UPDATE ---\n",
        "        title_tag = soup.select_one(LINKEDIN_TITLE_SELECTOR)\n",
        "        if title_tag: metadata[\"title\"] = clean_text(title_tag.get_text())\n",
        "\n",
        "        # --- Using EXAMPLE selectors - Try multiple patterns for Bio ---\n",
        "        bio = None\n",
        "        og_bio_tag = soup.select_one(SOCIAL_BIO_SELECTOR_OG)\n",
        "        if og_bio_tag and og_bio_tag.has_attr('content'): bio = clean_text(og_bio_tag['content'])\n",
        "        if not bio:\n",
        "            meta_bio_tag = soup.select_one(SOCIAL_BIO_SELECTOR_META)\n",
        "            if meta_bio_tag and meta_bio_tag.has_attr('content'): bio = clean_text(meta_bio_tag['content'])\n",
        "        metadata[\"bio_snippet\"] = bio\n",
        "\n",
        "    except Exception as e: print(f\"Error parsing metadata from {profile_url}: {e}\")\n",
        "    return metadata\n",
        "\n",
        "\n",
        "# --- Reddit Parsing Function ---\n",
        "def parse_reddit_findings(reddit_results):\n",
        "    \"\"\"Analyzes Reddit results for common themes/strategies.\"\"\"\n",
        "    notes = []\n",
        "    has_linkedin_mention = False; has_official_address_mention = False\n",
        "    for result in reddit_results:\n",
        "        content = (result.get(\"title\", \"\") + \" \" + result.get(\"snippet\", \"\")).lower()\n",
        "        if \"linkedin\" in content and (\"program director\" in content or \"music director\" in content or \"contact\" in content):\n",
        "            if not has_linkedin_mention: notes.append(\"Reddit users recommend using LinkedIn to find PDs/MDs.\"); has_linkedin_mention = True\n",
        "        if \"1221 avenue of the americas\" in content or \"official mail\" in content:\n",
        "             if not has_official_address_mention: notes.append(\"Reddit confirms official NYC mail-in address.\"); has_official_address_mention = True\n",
        "        if \"email format\" in content or re.search(r'\\b[a-z0-9._%+-]+@[a-z0-9.-]+\\.com\\b', content): # Broadened regex slightly\n",
        "            # Only add note once, avoid specific speculative formats\n",
        "             if \"speculation\" not in \" \".join(notes): notes.append(\"Reddit may contain *speculation* on email formats (treat with extreme caution).\")\n",
        "    return \" \".join(notes) if notes else \"No specific strategy insights found in Reddit search results.\"\n",
        "\n",
        "\n",
        "# --- Main Execution Logic ---\n",
        "def main():\n",
        "    \"\"\"Main function to orchestrate the scraping and data export.\"\"\"\n",
        "    all_data = []; processed_linkedin_urls = set(); processed_count = 0\n",
        "\n",
        "    # --- Load Targets ---\n",
        "    try:\n",
        "        print(f\"Loading targets from {INPUT_CSV_FILE}...\")\n",
        "        targets_df = pd.read_csv(INPUT_CSV_FILE); targets = targets_df.to_dict('records')\n",
        "        print(f\"Loaded {len(targets)} targets.\")\n",
        "    except FileNotFoundError: print(f\"Error: Input file '{INPUT_CSV_FILE}' not found.\"); return\n",
        "    except Exception as e: print(f\"Error reading input file {INPUT_CSV_FILE}: {e}\"); return\n",
        "\n",
        "    print(\"\\n--- Starting Processing ---\")\n",
        "    # --- Process Targets Loop ---\n",
        "    for target in targets:\n",
        "        target_name = target.get(\"Name\"); target_role_type = target.get(\"Role_Type\"); target_channel = target.get(\"Associated_Channel\")\n",
        "        if not target_role_type: print(f\"Skipping row with missing 'Role_Type': {target}\"); continue\n",
        "        print(f\"\\nProcessing Target: Name='{target_name}', Role='{target_role_type}', Channel='{target_channel}'\")\n",
        "\n",
        "        # --- Construct Search Query ---\n",
        "        query_parts = []\n",
        "        if target_name: query_parts.append(f'\"{target_name}\"')\n",
        "        # If Role_Type is 'Role', the Name column likely holds the actual role title\n",
        "        if target_role_type == \"Role\" and target_name: query_parts = [f'\"{target_name}\"']\n",
        "        # Don't add Role_Type if it was already used as the Name\n",
        "        elif target_role_type and target_role_type != \"Role\": query_parts.append(f'\"{target_role_type}\"')\n",
        "        query_parts.append(\"SiriusXM\")\n",
        "        if target_channel: query_parts.append(f'\"{target_channel}\"')\n",
        "        search_query = \" \".join(query_parts)\n",
        "\n",
        "        # --- Search Engine Lookup ---\n",
        "        found_results = search_engine_lookup(search_query)\n",
        "\n",
        "        # --- Process Found URLs ---\n",
        "        linkedin_url = None; twitter_url = None; instagram_url = None; other_urls = []\n",
        "        for result_item in found_results:\n",
        "            url = result_item.get('link')\n",
        "            if not url: continue # Skip if no URL in this result item\n",
        "\n",
        "            # Assign URLs (take first found for each type)\n",
        "            if (\"linkedin.com/in/\" in url or \"linkedin.com/pub/\" in url) and not linkedin_url: linkedin_url = url\n",
        "            elif \"twitter.com/\" in url and not twitter_url: twitter_url = url\n",
        "            elif \"instagram.com/\" in url and not instagram_url: instagram_url = url\n",
        "            elif any(domain in url for domain in [\"facebook.com/\", \"tiktok.com/\", \"siriusxm.com/hosts/\"]): other_urls.append(url)\n",
        "\n",
        "        # --- Deduplication Check ---\n",
        "        if linkedin_url and linkedin_url in processed_linkedin_urls:\n",
        "            print(f\"Skipping duplicate LinkedIn profile: {linkedin_url}\")\n",
        "            continue\n",
        "\n",
        "        # --- Prepare Data Entry ---\n",
        "        entry = {\n",
        "            \"Name\": target_name, \"Role_Type\": target_role_type, \"Associated_Channel\": target_channel,\n",
        "            \"LinkedIn_URL\": linkedin_url, \"LinkedIn_Title\": None, \"Twitter_URL\": twitter_url,\n",
        "            \"Instagram_URL\": instagram_url, \"Other_Social_URL\": \"; \".join(other_urls) if other_urls else None,\n",
        "            \"Social_Bio_Snippet\": None, \"Reddit_Insights\": None, \"Notes\": \"Requires manual verification.\"\n",
        "        }\n",
        "\n",
        "        # --- Metadata Scraping ---\n",
        "        if linkedin_url:\n",
        "            linkedin_meta = scrape_profile_metadata(linkedin_url)\n",
        "            entry[\"LinkedIn_Title\"] = linkedin_meta.get(\"title\")\n",
        "            processed_linkedin_urls.add(linkedin_url) # Add only if scrape attempted\n",
        "            # Refine name based on LinkedIn title if input name was blank or generic role title\n",
        "            if (not entry[\"Name\"] or entry[\"Name\"].lower() in [\"program director\", \"music director\", \"host\", \"dj\", \"curator\"]) and entry[\"LinkedIn_Title\"]:\n",
        "                 title_parts = re.split(r'\\s*-\\s*|\\s*\\|\\s*', entry[\"LinkedIn_Title\"], 1)\n",
        "                 if title_parts and title_parts[0].strip(): entry[\"Name\"] = title_parts[0].strip() # Update name\n",
        "\n",
        "        social_bio = None\n",
        "        if twitter_url:\n",
        "             twitter_meta = scrape_profile_metadata(twitter_url); social_bio = twitter_meta.get(\"bio_snippet\")\n",
        "        if instagram_url and not social_bio:\n",
        "             ig_meta = scrape_profile_metadata(instagram_url); social_bio = ig_meta.get(\"bio_snippet\")\n",
        "        entry[\"Social_Bio_Snippet\"] = social_bio\n",
        "\n",
        "        # --- Reddit Context ---\n",
        "        reddit_search_query_parts = [\"SiriusXM\"]\n",
        "        if entry[\"Name\"]: reddit_search_query_parts.append(f'\"{entry[\"Name\"]}\"') # Use potentially updated name\n",
        "        if target_channel: reddit_search_query_parts.append(f'\"{target_channel}\"')\n",
        "        reddit_search_query_parts.append(\"contact OR email OR submit\")\n",
        "        reddit_search_results = search_reddit(\" \".join(reddit_search_query_parts))\n",
        "        entry[\"Reddit_Insights\"] = parse_reddit_findings(reddit_search_results)\n",
        "\n",
        "        # --- Store Entry & Update Count ---\n",
        "        all_data.append(entry)\n",
        "        processed_count += 1\n",
        "        print(f\"Processed entry #{processed_count} for '{entry['Name']}'\")\n",
        "\n",
        "        # --- Periodic Saving ---\n",
        "        if processed_count > 0 and processed_count % SAVE_INTERVAL == 0:\n",
        "            print(f\"\\n--- Saving progress ({processed_count} records processed) to {OUTPUT_CSV_FILE} ---\")\n",
        "            try:\n",
        "                # Create DataFrame from current data\n",
        "                temp_df = pd.DataFrame(all_data)\n",
        "                # Define columns explicitly for consistent saving\n",
        "                final_columns = [\"Name\", \"Role_Type\", \"Associated_Channel\", \"LinkedIn_URL\", \"LinkedIn_Title\", \"Twitter_URL\", \"Instagram_URL\", \"Other_Social_URL\", \"Social_Bio_Snippet\", \"Reddit_Insights\", \"Notes\"]\n",
        "                # Add any missing columns with None/NaN before saving\n",
        "                for col in final_columns:\n",
        "                    if col not in temp_df.columns: temp_df[col] = None\n",
        "                temp_df = temp_df[final_columns] # Reorder\n",
        "                temp_df.to_csv(OUTPUT_CSV_FILE, index=False, encoding='utf-8')\n",
        "                print(\"--- Progress saved successfully. ---\")\n",
        "            except Exception as e: print(f\"--- Error saving progress: {e} ---\")\n",
        "            print(\"-\" * 20) # Separator\n",
        "\n",
        "    # --- Final Export ---\n",
        "    print(\"\\n--- Processing Complete ---\")\n",
        "    if not all_data: print(\"No data collected.\"); return\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "    final_columns = [\"Name\", \"Role_Type\", \"Associated_Channel\", \"LinkedIn_URL\", \"LinkedIn_Title\", \"Twitter_URL\", \"Instagram_URL\", \"Other_Social_URL\", \"Social_Bio_Snippet\", \"Reddit_Insights\", \"Notes\"]\n",
        "    for col in final_columns:\n",
        "        if col not in df.columns: df[col] = None\n",
        "    df = df[final_columns]\n",
        "    try:\n",
        "        df.to_csv(OUTPUT_CSV_FILE, index=False, encoding='utf-8')\n",
        "        print(f\"Final data exported successfully to {OUTPUT_CSV_FILE} ({len(df)} records)\")\n",
        "    except Exception as e: print(f\"Error exporting final data to CSV: {e}\")\n",
        "\n",
        "# --- Run the Script ---\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}